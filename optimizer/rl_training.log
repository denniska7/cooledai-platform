/usr/local/lib/python3.11/site-packages/gymnasium/spaces/box.py:235: UserWarning: [33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64[0m
  gym.logger.warn(
/usr/local/lib/python3.11/site-packages/gymnasium/spaces/box.py:305: UserWarning: [33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64[0m
  gym.logger.warn(
/usr/local/lib/python3.11/site-packages/stable_baselines3/common/env_checker.py:461: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf. https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html
  warnings.warn(

üñ•Ô∏è  Using device: CPU
   (RL training uses CPU for stability; RecurrentPINN uses MPS)
======================================================================
TRAINING AUTONOMOUS COOLING CONTROL AGENT
======================================================================

üì¶ Creating environment...
Loading RecurrentPINN from checkpoints/best_recurrent_pinn.pt...
RecurrentPINN loaded successfully!
‚úì Checking environment validity...

‚úì Environment created successfully!
  Observation space: Box([  0.    0.    0.5 -10.    0.    0. ], [1.e+02 2.e+05 3.e+00 1.e+01 1.e+02 1.e+02], (6,), float32)
  Action space: Box(-0.5, 0.5, (1,), float32)

ü§ñ Initializing PPO agent...
Using cpu device
Wrapping the env in a DummyVecEnv.
‚úì PPO agent initialized!
  Policy network: MLP (Multi-Layer Perceptron)
  Learning rate: 3e-4
  Total timesteps: 100,000

======================================================================
STARTING TRAINING
======================================================================
Training for 100,000 timesteps...
This will take approximately 30-60 minutes depending on hardware.

---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    fps             | 673      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 1.78e+03    |
| time/                   |             |
|    fps                  | 589         |
|    iterations           | 2           |
|    time_elapsed         | 6           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.016422115 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 196         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0253     |
|    std                  | 0.964       |
|    value_loss           | 499         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 1.99e+03    |
| time/                   |             |
|    fps                  | 571         |
|    iterations           | 3           |
|    time_elapsed         | 10          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.012170143 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 562         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0212     |
|    std                  | 0.955       |
|    value_loss           | 1.36e+03    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 2.3e+03     |
| time/                   |             |
|    fps                  | 559         |
|    iterations           | 4           |
|    time_elapsed         | 14          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.009292945 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 555         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0152     |
|    std                  | 0.936       |
|    value_loss           | 1.4e+03     |
-----------------------------------------

üìä Episode 10:
  Avg Reward: 2514.25
  Avg Max Temp: 55.13¬∞C
  Avg Violations: 0.00
  Avg Energy: 322.15
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 2.51e+03   |
| time/                   |            |
|    fps                  | 558        |
|    iterations           | 5          |
|    time_elapsed         | 18         |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.00941968 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 927        |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0142    |
|    std                  | 0.926      |
|    value_loss           | 2.19e+03   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 2.66e+03    |
| time/                   |             |
|    fps                  | 554         |
|    iterations           | 6           |
|    time_elapsed         | 22          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.007311277 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 964         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0105     |
|    std                  | 0.913       |
|    value_loss           | 2.27e+03    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 2.77e+03    |
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 7           |
|    time_elapsed         | 25          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.009702837 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 947         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0107     |
|    std                  | 0.897       |
|    value_loss           | 2.18e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 2.85e+03     |
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 8            |
|    time_elapsed         | 29           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.0027174193 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 961          |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00061     |
|    std                  | 0.883        |
|    value_loss           | 2.19e+03     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 2.87e+03     |
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 9            |
|    time_elapsed         | 33           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0010157478 |
|    clip_fraction        | 0.00537      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 832          |
|    n_updates            | 80           |
|    policy_gradient_loss | 0.000128     |
|    std                  | 0.88         |
|    value_loss           | 1.88e+03     |
------------------------------------------

üìä Episode 20:
  Avg Reward: 3335.13
  Avg Max Temp: 55.64¬∞C
  Avg Violations: 0.00
  Avg Energy: 35.87
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 2.92e+03      |
| time/                   |               |
|    fps                  | 545           |
|    iterations           | 10            |
|    time_elapsed         | 37            |
|    total_timesteps      | 20480         |
| train/                  |               |
|    approx_kl            | 0.00088821247 |
|    clip_fraction        | 0.0124        |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.29         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 701           |
|    n_updates            | 90            |
|    policy_gradient_loss | -0.000694     |
|    std                  | 0.874         |
|    value_loss           | 1.63e+03      |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 2.97e+03     |
| time/                   |              |
|    fps                  | 543          |
|    iterations           | 11           |
|    time_elapsed         | 41           |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0060020294 |
|    clip_fraction        | 0.0772       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 817          |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00392     |
|    std                  | 0.872        |
|    value_loss           | 1.92e+03     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 2.97e+03     |
| time/                   |              |
|    fps                  | 543          |
|    iterations           | 12           |
|    time_elapsed         | 45           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0054959115 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.27        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 601          |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00161     |
|    std                  | 0.859        |
|    value_loss           | 1.54e+03     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 2.97e+03     |
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 13           |
|    time_elapsed         | 49           |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0014906721 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 486          |
|    n_updates            | 120          |
|    policy_gradient_loss | 0.000362     |
|    std                  | 0.846        |
|    value_loss           | 1.19e+03     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.01e+03     |
| time/                   |              |
|    fps                  | 535          |
|    iterations           | 14           |
|    time_elapsed         | 53           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0023664199 |
|    clip_fraction        | 0.00776      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 627          |
|    n_updates            | 130          |
|    policy_gradient_loss | 5.03e-05     |
|    std                  | 0.858        |
|    value_loss           | 1.45e+03     |
------------------------------------------

üìä Episode 30:
  Avg Reward: 3270.69
  Avg Max Temp: 54.65¬∞C
  Avg Violations: 0.00
  Avg Energy: 21.91
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.04e+03     |
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 15           |
|    time_elapsed         | 57           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0058005974 |
|    clip_fraction        | 0.0362       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.27        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 699          |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00126     |
|    std                  | 0.858        |
|    value_loss           | 1.58e+03     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 3.04e+03    |
| time/                   |             |
|    fps                  | 535         |
|    iterations           | 16          |
|    time_elapsed         | 61          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.002530443 |
|    clip_fraction        | 0.0286      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 651         |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.000238   |
|    std                  | 0.853       |
|    value_loss           | 1.51e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.06e+03     |
| time/                   |              |
|    fps                  | 536          |
|    iterations           | 17           |
|    time_elapsed         | 64           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0033844784 |
|    clip_fraction        | 0.0164       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 391          |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.000425    |
|    std                  | 0.853        |
|    value_loss           | 950          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 3.08e+03    |
| time/                   |             |
|    fps                  | 537         |
|    iterations           | 18          |
|    time_elapsed         | 68          |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.000833493 |
|    clip_fraction        | 0.0291      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 560         |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00162    |
|    std                  | 0.839       |
|    value_loss           | 1.35e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.11e+03     |
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 19           |
|    time_elapsed         | 72           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0013264287 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 539          |
|    n_updates            | 180          |
|    policy_gradient_loss | 7.94e-05     |
|    std                  | 0.836        |
|    value_loss           | 1.28e+03     |
------------------------------------------

üìä Episode 40:
  Avg Reward: 3374.86
  Avg Max Temp: 57.63¬∞C
  Avg Violations: 0.00
  Avg Energy: 20.54
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 3.12e+03   |
| time/                   |            |
|    fps                  | 537        |
|    iterations           | 20         |
|    time_elapsed         | 76         |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.00639602 |
|    clip_fraction        | 0.0716     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 1.19e-07   |
|    learning_rate        | 0.0003     |
|    loss                 | 490        |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0032    |
|    std                  | 0.833      |
|    value_loss           | 1.19e+03   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 3.1e+03     |
| time/                   |             |
|    fps                  | 538         |
|    iterations           | 21          |
|    time_elapsed         | 79          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.003897902 |
|    clip_fraction        | 0.0218      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 309         |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.000665   |
|    std                  | 0.825       |
|    value_loss           | 740         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 3.1e+03     |
| time/                   |             |
|    fps                  | 539         |
|    iterations           | 22          |
|    time_elapsed         | 83          |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.002285561 |
|    clip_fraction        | 0.00664     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 248         |
|    n_updates            | 210         |
|    policy_gradient_loss | 0.000106    |
|    std                  | 0.828       |
|    value_loss           | 663         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.07e+03     |
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 23           |
|    time_elapsed         | 87           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0042142975 |
|    clip_fraction        | 0.0299       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 266          |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.0019      |
|    std                  | 0.824        |
|    value_loss           | 610          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.05e+03     |
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 24           |
|    time_elapsed         | 90           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0010040113 |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.21        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 61           |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00102     |
|    std                  | 0.799        |
|    value_loss           | 201          |
------------------------------------------

üìä Episode 50:
  Avg Reward: 2778.52
  Avg Max Temp: 50.12¬∞C
  Avg Violations: 0.00
  Avg Energy: 18.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 3.06e+03    |
| time/                   |             |
|    fps                  | 542         |
|    iterations           | 25          |
|    time_elapsed         | 94          |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.007296613 |
|    clip_fraction        | 0.0895      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 78.8        |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00405    |
|    std                  | 0.795       |
|    value_loss           | 220         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.08e+03     |
| time/                   |              |
|    fps                  | 542          |
|    iterations           | 26           |
|    time_elapsed         | 98           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0028370162 |
|    clip_fraction        | 0.0324       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.19        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 324          |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.000849    |
|    std                  | 0.795        |
|    value_loss           | 822          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.09e+03     |
| time/                   |              |
|    fps                  | 543          |
|    iterations           | 27           |
|    time_elapsed         | 101          |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0018920313 |
|    clip_fraction        | 0.00439      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.19        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 306          |
|    n_updates            | 260          |
|    policy_gradient_loss | 0.000164     |
|    std                  | 0.796        |
|    value_loss           | 763          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.11e+03     |
| time/                   |              |
|    fps                  | 544          |
|    iterations           | 28           |
|    time_elapsed         | 105          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0028082272 |
|    clip_fraction        | 0.0348       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.19        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 275          |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00178     |
|    std                  | 0.791        |
|    value_loss           | 707          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.12e+03     |
| time/                   |              |
|    fps                  | 545          |
|    iterations           | 29           |
|    time_elapsed         | 108          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0007597146 |
|    clip_fraction        | 0.0463       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.19        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 252          |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00134     |
|    std                  | 0.793        |
|    value_loss           | 655          |
------------------------------------------

üìä Episode 60:
  Avg Reward: 3483.25
  Avg Max Temp: 56.20¬∞C
  Avg Violations: 0.00
  Avg Energy: 16.15
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 3.13e+03   |
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 30         |
|    time_elapsed         | 112        |
|    total_timesteps      | 61440      |
| train/                  |            |
|    approx_kl            | 0.00853529 |
|    clip_fraction        | 0.0709     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 222        |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.00152   |
|    std                  | 0.799      |
|    value_loss           | 604        |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.13e+03     |
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 31           |
|    time_elapsed         | 116          |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0012161426 |
|    clip_fraction        | 0.00942      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.2         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 167          |
|    n_updates            | 300          |
|    policy_gradient_loss | 0.000561     |
|    std                  | 0.803        |
|    value_loss           | 449          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.14e+03     |
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 32           |
|    time_elapsed         | 119          |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0017630887 |
|    clip_fraction        | 0.00664      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.21        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 135          |
|    n_updates            | 310          |
|    policy_gradient_loss | 0.000352     |
|    std                  | 0.823        |
|    value_loss           | 380          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.15e+03     |
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 33           |
|    time_elapsed         | 123          |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0073471954 |
|    clip_fraction        | 0.0398       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 170          |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.000783    |
|    std                  | 0.838        |
|    value_loss           | 462          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.16e+03     |
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 34           |
|    time_elapsed         | 127          |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0020239986 |
|    clip_fraction        | 0.0569       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 147          |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.00245     |
|    std                  | 0.826        |
|    value_loss           | 417          |
------------------------------------------

üìä Episode 70:
  Avg Reward: 3384.33
  Avg Max Temp: 60.22¬∞C
  Avg Violations: 0.00
  Avg Energy: 15.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.17e+03     |
| time/                   |              |
|    fps                  | 547          |
|    iterations           | 35           |
|    time_elapsed         | 130          |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0017471826 |
|    clip_fraction        | 0.0405       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 135          |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.000534    |
|    std                  | 0.833        |
|    value_loss           | 379          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 3.15e+03    |
| time/                   |             |
|    fps                  | 547         |
|    iterations           | 36          |
|    time_elapsed         | 134         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.004288204 |
|    clip_fraction        | 0.0314      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 79.7        |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00108    |
|    std                  | 0.831       |
|    value_loss           | 229         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.14e+03     |
| time/                   |              |
|    fps                  | 548          |
|    iterations           | 37           |
|    time_elapsed         | 138          |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0029532001 |
|    clip_fraction        | 0.0807       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 0.112        |
|    n_updates            | 360          |
|    policy_gradient_loss | 0.00111      |
|    std                  | 0.837        |
|    value_loss           | 3.31         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.15e+03     |
| time/                   |              |
|    fps                  | 548          |
|    iterations           | 38           |
|    time_elapsed         | 141          |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0032460743 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 87           |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.000451    |
|    std                  | 0.838        |
|    value_loss           | 249          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.15e+03     |
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 39           |
|    time_elapsed         | 145          |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 1.955044e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 77.2         |
|    n_updates            | 380          |
|    policy_gradient_loss | -1.12e-05    |
|    std                  | 0.837        |
|    value_loss           | 245          |
------------------------------------------

üìä Episode 80:
  Avg Reward: 3085.72
  Avg Max Temp: 52.43¬∞C
  Avg Violations: 0.00
  Avg Energy: 13.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.15e+03     |
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 40           |
|    time_elapsed         | 149          |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0033997619 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 46.7         |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.0018      |
|    std                  | 0.837        |
|    value_loss           | 112          |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1e+03         |
|    ep_rew_mean          | 3.14e+03      |
| time/                   |               |
|    fps                  | 549           |
|    iterations           | 41            |
|    time_elapsed         | 152           |
|    total_timesteps      | 83968         |
| train/                  |               |
|    approx_kl            | 0.00050083076 |
|    clip_fraction        | 0.00225       |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.24         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 35.7          |
|    n_updates            | 400           |
|    policy_gradient_loss | -0.000392     |
|    std                  | 0.837         |
|    value_loss           | 99.8          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.14e+03     |
| time/                   |              |
|    fps                  | 548          |
|    iterations           | 42           |
|    time_elapsed         | 156          |
|    total_timesteps      | 86016        |
| train/                  |              |
|    approx_kl            | 0.0046644593 |
|    clip_fraction        | 0.0395       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.56         |
|    n_updates            | 410          |
|    policy_gradient_loss | 0.000894     |
|    std                  | 0.826        |
|    value_loss           | 7.61         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.14e+03     |
| time/                   |              |
|    fps                  | 547          |
|    iterations           | 43           |
|    time_elapsed         | 160          |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0017618878 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 45.6         |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.000718    |
|    std                  | 0.825        |
|    value_loss           | 161          |
------------------------------------------

üìä Episode 90:
  Avg Reward: 2986.73
  Avg Max Temp: 50.68¬∞C
  Avg Violations: 0.00
  Avg Energy: 13.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.13e+03     |
| time/                   |              |
|    fps                  | 547          |
|    iterations           | 44           |
|    time_elapsed         | 164          |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 0.0016704154 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 35.4         |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00127     |
|    std                  | 0.82         |
|    value_loss           | 81.4         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 3.14e+03    |
| time/                   |             |
|    fps                  | 547         |
|    iterations           | 45          |
|    time_elapsed         | 168         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.001863735 |
|    clip_fraction        | 0.0201      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 34.4        |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.000711   |
|    std                  | 0.821       |
|    value_loss           | 80          |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.14e+03     |
| time/                   |              |
|    fps                  | 547          |
|    iterations           | 46           |
|    time_elapsed         | 172          |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.0028425097 |
|    clip_fraction        | 0.0184       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 23           |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.000162    |
|    std                  | 0.821        |
|    value_loss           | 93.9         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | 3.14e+03     |
| time/                   |              |
|    fps                  | 547          |
|    iterations           | 47           |
|    time_elapsed         | 175          |
|    total_timesteps      | 96256        |
| train/                  |              |
|    approx_kl            | 0.0015095908 |
|    clip_fraction        | 0.00747      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 34.5         |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.000242    |
|    std                  | 0.814        |
|    value_loss           | 72.3         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 3.14e+03    |
| time/                   |             |
|    fps                  | 547         |
|    iterations           | 48          |
|    time_elapsed         | 179         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.002445515 |
|    clip_fraction        | 0.00762     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 34.7        |
|    n_updates            | 470         |
|    policy_gradient_loss | 0.000479    |
|    std                  | 0.81        |
|    value_loss           | 69.1        |
-----------------------------------------

üìä Episode 100:
  Avg Reward: 3286.30
  Avg Max Temp: 56.54¬∞C
  Avg Violations: 0.00
  Avg Energy: 13.50
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 3.15e+03    |
| time/                   |             |
|    fps                  | 546         |
|    iterations           | 49          |
|    time_elapsed         | 183         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.002681159 |
|    clip_fraction        | 0.0061      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 9.68        |
|    n_updates            | 480         |
|    policy_gradient_loss | -2.35e-05   |
|    std                  | 0.81        |
|    value_loss           | 59.3        |
-----------------------------------------

‚úì Training statistics saved to optimizer/training_stats.json

üíæ Saving trained agent to optimizer/ppo_cooling_agent...
‚úì Agent saved successfully!

======================================================================
FINAL EVALUATION
======================================================================

Evaluating agent over 10 episodes...
  Episode 1: Reward=2487.36, MaxTemp=46.91¬∞C, Violations=0
  Episode 2: Reward=3487.43, MaxTemp=58.57¬∞C, Violations=0
  Episode 3: Reward=3487.19, MaxTemp=55.17¬∞C, Violations=0
  Episode 4: Reward=3487.49, MaxTemp=61.13¬∞C, Violations=0
  Episode 5: Reward=2487.23, MaxTemp=47.76¬∞C, Violations=0
  Episode 6: Reward=2487.13, MaxTemp=49.77¬∞C, Violations=0
  Episode 7: Reward=2487.48, MaxTemp=45.26¬∞C, Violations=0
  Episode 8: Reward=3487.36, MaxTemp=63.09¬∞C, Violations=0
  Episode 9: Reward=3487.17, MaxTemp=50.45¬∞C, Violations=0
  Episode 10: Reward=3487.46, MaxTemp=60.55¬∞C, Violations=0

üìä Evaluation Summary:
  Average Reward: 3087.33 ¬± 489.92
  Average Length: 1000 steps
  Average Max Temp: 53.87¬∞C ¬± 6.28
  Average Violations: 0.00
  Average Energy: 12.67
  Success Rate: 100.0%

======================================================================
üéâ TRAINING COMPLETE!
======================================================================

Trained agent saved to: optimizer/ppo_cooling_agent.zip
Training stats saved to: optimizer/training_stats.json

Next steps:
  1. Run 'python3.11 optimizer/deploy_control_policy.py' to test the agent
  2. Use the agent for real-time cooling control
